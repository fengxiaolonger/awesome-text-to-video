<div align="center">

# üé¨ Awesome Text-to-Video

**A comprehensive, community-curated collection of AI video generation tools, prompts, workflows, open-source models, and learning resources.**

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![Entries](https://img.shields.io/badge/entries-200+-blue?style=flat-square)
![Last Update](https://img.shields.io/badge/updated-February%202026-brightgreen?style=flat-square)
[![License: CC0-1.0](https://img.shields.io/badge/License-CC0_1.0-lightgrey.svg)](http://creativecommons.org/publicdomain/zero/1.0/)

<br/>

The AI video generation space moves fast. New models drop weekly, pricing changes overnight, and yesterday's state-of-the-art becomes today's baseline. This list exists to help creators, developers, and researchers navigate the landscape without drowning in hype.

Everything here is verified by contributors who actually use these tools.

<br/>

[Submit a Resource](https://github.com/fengxiaolonger/awesome-text-to-video/issues/new) ¬∑ [Report Issue](https://github.com/fengxiaolonger/awesome-text-to-video/issues) ¬∑ [Contribute](CONTRIBUTING.md)

</div>

---

## Table of Contents

| | | |
|---|---|---|
| [Multi-Model Platforms](#multi-model-platforms) (8) | [Text-to-Video Tools](#text-to-video-tools) (22) | [Image-to-Video Tools](#image-to-video-tools) (18) |
| [Video Editing & Post-Production](#video-editing--post-production) (15) | [AI Avatar & Talking Head](#ai-avatar--talking-head) (12) | [Music Video & Audio-Reactive](#music-video--audio-reactive) (8) |
| [Open Source Models](#open-source-models) (20) | [ComfyUI Workflows](#comfyui-workflows) (15) | [APIs & SDKs](#apis--sdks) (14) |
| [Prompt Engineering](#prompt-engineering) (25) | [Benchmark & Comparisons](#benchmark--comparisons) (6) | [Research Papers](#research-papers) (30) |
| [Tutorials & Courses](#tutorials--courses) (18) | [Communities & Forums](#communities--forums) (12) | [Multi-Language Resources](#multi-language-resources) (18) |

---

## Multi-Model Platforms

Platforms that aggregate multiple AI video models into a single interface, so you can compare outputs and pick the best result without juggling accounts.

- [HeyVid AI](https://heyvid.ai) ‚Äî Aggregates 20+ models (Kling, Veo 3, Sora 2, Runway, Seedance, Hailuo, Pika, PixVerse, Vidu, Luma, Wan, Hunyuan, Midjourney, Flux, GPT-4o, DALL-E, Recraft, Ideogram, SD, Qwen). Supports text-to-video, image-to-video, video-to-video, text-to-image, TTS, and AI music. Free tier available. [18 languages supported](https://heyvid.ai).
- [Replicate](https://replicate.com) ‚Äî Run open-source models via API. Hosts Wan 2.1, CogVideoX, Stable Video Diffusion, AnimateDiff, LTX-Video, and 100+ video models. Pay-per-second billing.
- [fal.ai](https://fal.ai) ‚Äî Fast inference platform for generative models. Supports Kling, Minimax, Wan, LTX-Video, Hunyuan with optimized GPU routing. Developer-focused.
- [Together AI](https://www.together.ai) ‚Äî Serverless and dedicated endpoints for open-source video models. Good for batch processing and fine-tuning workflows.
- [Novita AI](https://novita.ai) ‚Äî Multi-model API platform with video generation, image generation, and LLM inference. Supports Wan 2.1, HunyuanVideo, AnimateDiff.
- [ComfyDeploy](https://www.comfydeploy.com) ‚Äî Deploy ComfyUI workflows as APIs. Turn any custom video generation pipeline into a production endpoint.
- [Segmind](https://www.segmind.com) ‚Äî Serverless APIs for generative AI. Supports multiple video models with competitive pricing and workflow chaining.
- [Modelslab](https://modelslab.com) ‚Äî API access to video generation models including Stable Video Diffusion, AnimateDiff, and custom fine-tuned models.

## Text-to-Video Tools

Generate video directly from text prompts. Quality, length, and style vary significantly between tools.

### Frontier Models (Best Quality)

- [Google Veo 2](https://deepmind.google/technologies/veo/) ‚Äî Google's flagship. 4K output, up to 8s, strong physical understanding. Available via Google AI Studio and Vertex AI. Currently the best at complex camera movements.
- [OpenAI Sora 2](https://openai.com/sora) ‚Äî Up to 20s at 1080p. Strong narrative coherence across frames. Available to ChatGPT Plus/Pro subscribers. Storyboard mode for multi-shot sequences.
- [Kling 2.0](https://klingai.com) ‚Äî Kuaishou's model. Up to 10s, 1080p. Excellent at human motion and facial expressions. Available globally via web and API.
- [Runway Gen-3 Alpha](https://runwayml.com) ‚Äî 10s clips with Motion Brush for fine-grained control. Strong creative community. $12/mo starter plan.
- [Minimax / Hailuo](https://hailuoai.video) ‚Äî 6s at 1080p. Known for cinematic quality and good text rendering. Free tier generous for testing.
- [Luma Dream Machine 2.0](https://lumalabs.ai/dream-machine) ‚Äî 5s clips. Strong at 3D understanding and object permanence. Fast generation times (~30s per clip).
- [Pika 2.0](https://pika.art) ‚Äî Scene ingredients system for precise control. Lip sync, sound effects generation. Good balance of quality and usability.

### Mid-Range Tools

- [PixVerse V3.5](https://pixverse.ai) ‚Äî 8s at 1080p. Good motion quality. Specializes in anime/stylized content. Free daily credits.
- [Vidu 2.0](https://www.vidu.com) ‚Äî Shengshu Technology. 8s at 1080p. Strong at Chinese-style aesthetics. Multi-subject reference support.
- [Seedance](https://seedance.ai) ‚Äî ByteDance's latest. Character-consistent generation with dance/motion specialization.
- [Wan 2.1](https://wan.video) ‚Äî Alibaba's open+commercial model. 5s at 720p for free tier. Full model weights available for self-hosting.
- [Genmo Mochi](https://www.genmo.ai) ‚Äî Open-source backbone, 5s at 720p. Good for experimentation and fine-tuning. Free tier available.
- [Haiper 2.0](https://haiper.ai) ‚Äî 6s at 1080p. Strong at stylized content. Founded by ex-Google DeepMind researchers.
- [Hunyuan Video](https://video.hunyuan.tencent.com) ‚Äî Tencent's model. Open-source weights available. Good Chinese text rendering.
- [Jimeng AI](https://jimeng.jianying.com) ‚Äî ByteDance's creative platform. Text-to-video with strong template system. Primarily Chinese market.

### Specialized Tools

- [Fliki](https://fliki.ai) ‚Äî Blog-to-video and script-to-video with 2000+ AI voices. Best for content marketing and social media. $28/mo.
- [Pictory](https://pictory.ai) ‚Äî Long-form video from scripts/articles. Auto-highlights and summarization. Best for repurposing written content.
- [InVideo](https://invideo.io) ‚Äî Template-based with AI script writing. 5000+ templates. Good for marketing videos and ads.
- [Synthesia](https://www.synthesia.io) ‚Äî AI avatar-based video generation. 230+ avatars, 140+ languages. Enterprise-focused, best for training/corporate videos. $22/mo.
- [Steve.AI](https://steve.ai) ‚Äî Script to animated video. Patent-pending AI technology. Good for explainer videos and presentations.

## Image-to-Video Tools

Animate static images. Quality depends heavily on the source image and the model's motion understanding.

### Best Overall

- [HeyVid AI](https://heyvid.ai) ‚Äî Multi-model image-to-video with Kling, Runway, Hailuo, Pika, Luma in one interface. Compare outputs from different models side-by-side. [Try free](https://heyvid.ai).
- [Runway Gen-3 Alpha](https://runwayml.com) ‚Äî Motion Brush lets you paint motion onto specific regions. Best for precise creative control.
- [Kling 2.0](https://klingai.com) ‚Äî Exceptional at human motion from portraits. Natural cloth physics and hair movement.
- [Stable Video Diffusion](https://stability.ai) ‚Äî Open-source. Self-hostable. 4s at 576√ó1024. Good baseline for custom pipelines.

### Character Animation

- [D-ID](https://www.d-id.com) ‚Äî Talking head videos from a single photo. Real-time avatar streaming. API available. Best for customer-facing video.
- [HeyGen](https://www.heygen.com) ‚Äî AI spokesperson videos. 300+ voices, 40+ languages. Custom avatar training from 2min footage.
- [Hedra](https://www.hedra.com) ‚Äî Character-1 model for expressive talking heads. Good lip sync from audio.
- [LivePortrait](https://github.com/KwaiVGI/LivePortrait) ‚Äî Open-source portrait animation from Kuaishou. Stitching and retargeting pipeline. Self-hostable.

### Specialized Animation

- [Luma Dream Machine](https://lumalabs.ai) ‚Äî Strong at animating product shots and 3D objects. Physics-aware motion.
- [Pika](https://pika.art) ‚Äî Modify + Animate for targeted image changes before animation.
- [PixVerse](https://pixverse.ai) ‚Äî Good at anime-style animation. Character reference for consistent style.
- [Kaiber](https://kaiber.ai) ‚Äî Artistic transformations. Popular for music videos and abstract art animation. $5/mo.
- [Viggle](https://viggle.ai) ‚Äî Controllable character animation. Mix motion with character images.
- [Kling Motion Brush](https://klingai.com) ‚Äî Fine-grained motion control by brushing areas you want to animate.
- [DomoAI](https://www.domoai.app) ‚Äî Style transfer animation (cartoon, anime, 3D, pixel art). Discord-based interface.
- [ToonCrafter](https://github.com/ToonCrafter/ToonCrafter) ‚Äî Open-source cartoon interpolation. Generate in-between frames for hand-drawn animation.

## Video Editing & Post-Production

AI tools for editing, enhancing, and transforming existing video footage.

### All-in-One Editors

- [Runway](https://runwayml.com) ‚Äî Gen-3 inpainting, remove/replace objects, expand video canvas, color grade. The creative professional's toolkit.
- [Descript](https://www.descript.com) ‚Äî Edit video by editing its text transcript. Remove filler words automatically. AI green screen. $24/mo.
- [CapCut](https://www.capcut.com) ‚Äî Auto-captions, background removal, effects library. Free with Pro features. Best for short-form social content.
- [Veed.io](https://www.veed.io) ‚Äî Browser-based editor. Auto-subtitles in 100+ languages. Collaboration features. Good for teams.
- [FlexClip](https://www.flexclip.com) ‚Äî Template-driven creation with AI tools. Text-to-video, screen recorder. $9.99/mo.

### Enhancement & Upscaling

- [Topaz Video AI](https://www.topazlabs.com/topaz-video-ai) ‚Äî Industry-standard upscaling (up to 16K), deinterlacing, stabilization, frame interpolation. Desktop app, one-time purchase.
- [HitPaw Video Enhancer](https://www.hitpaw.com/video-enhancer.html) ‚Äî AI upscaling and denoising. Face enhancement model for old footage restoration.
- [CapCut Video Upscaler](https://www.capcut.com) ‚Äî Free browser-based upscaling. Good enough for social media content.

### Subtitles & Captions

- [Captions](https://www.captions.ai) ‚Äî AI-powered captions with animated styles. Trending caption templates for social media.
- [Submagic](https://www.submagic.co) ‚Äî Auto-captions with emoji integration. Batch processing for multiple videos.
- [Opus Clip](https://www.opus.pro) ‚Äî AI-powered long-to-short video repurposing. Auto-selects viral moments and adds captions.

### Background & Object Removal

- [Runway Inpainting](https://runwayml.com) ‚Äî Remove objects and fill with AI-generated content. Frame-by-frame consistency.
- [ProPainter](https://github.com/sczhou/ProPainter) ‚Äî Open-source video inpainting. Flow-guided propagation for temporal consistency. Self-hostable.

## AI Avatar & Talking Head

Generate presenter-style videos with AI-generated or cloned human avatars.

- [Synthesia](https://www.synthesia.io) ‚Äî 230+ stock avatars, custom avatar creation, 140+ languages. Enterprise API. SOC 2 compliant. The market leader.
- [HeyGen](https://www.heygen.com) ‚Äî Instant avatar from 2min video. Real-time streaming avatar. Multi-scene storyboard.
- [D-ID](https://www.d-id.com) ‚Äî Creative Reality Studio. Real-time API for conversational avatars. Used by customer service platforms.
- [Colossyan](https://www.colossyan.com) ‚Äî Workplace learning focused. Auto-translate videos into 70+ languages with lip sync.
- [Elai.io](https://elai.io) ‚Äî 80+ avatars. PowerPoint to video conversion. Collaboration features for teams.
- [DeepBrain AI](https://www.deepbrain.io) ‚Äî AI Studios platform. Real-time conversation with AI avatars. Kiosk deployment options.
- [Hour One](https://hourone.ai) ‚Äî Virtual human video creation. Focus on enterprise communications and L&D.
- [Vidnoz](https://www.vidnoz.com) ‚Äî 1000+ avatars, 140+ languages. Free tier with daily credits. Template library.
- [Renderforest](https://www.renderforest.com) ‚Äî Video maker with AI avatars, templates, and branding tools. All-in-one creative suite.
- [VEED AI Avatars](https://www.veed.io) ‚Äî Browser-based avatar creation. Integrate with VEED's editing suite.
- [Tavus](https://www.tavus.io) ‚Äî Personalized video at scale. Variable insertion for 1:1 outreach campaigns.
- [Rephrase.ai](https://www.rephrase.ai) ‚Äî Text-to-video API for personalized marketing. Dynamic video generation.

## Music Video & Audio-Reactive

Tools that generate or sync video to music and audio.

- [Kaiber](https://kaiber.ai) ‚Äî Upload audio, get AI-generated music video. Multiple style options (2D, 3D, anime). Used by major artists. $5/mo.
- [Neural Frames](https://www.neuralframes.com) ‚Äî AI music video generator. Stable Diffusion + audio-reactive motion. Prompt-per-beat control.
- [Deforum](https://deforum.github.io) ‚Äî Open-source audio-reactive Stable Diffusion animation. ComfyUI and A1111 integrations.
- [HeyVid AI Music](https://heyvid.ai) ‚Äî AI music generation (Suno integration) + video creation in one platform. Create soundtrack and visuals together.
- [Doodly](https://www.doodly.com) ‚Äî Whiteboard animation videos. Sync drawings to voiceover/music.
- [Vizzy](https://vizzy.io) ‚Äî Real-time music visualization. Live performance visuals from audio input.
- [Rotor Videos](https://rotorvideos.com) ‚Äî Automated music video creation from your tracks. Beat-synced editing.
- [Mootion](https://mootion.com) ‚Äî AI dance and character animation from audio. Text-to-motion generation.

## Open Source Models

Self-hostable models with publicly available weights. Sorted by release date (newest first).

### 2025-2026

- [Wan 2.1](https://github.com/Wan-Video/Wan2.1) ‚Äî **Alibaba** ‚Äî T2V and I2V, 480p-720p. 1.3B and 14B parameter versions. Apache 2.0. Currently the best open-source option for quality/accessibility balance. [Paper](https://arxiv.org/abs/2503.20314)
- [HunyuanVideo](https://github.com/Tencent/HunyuanVideo) ‚Äî **Tencent** ‚Äî 13B parameters, 720p. Full attention mechanism. Strong temporal consistency. Apache 2.0. [Paper](https://arxiv.org/abs/2412.03603)
- [LTX-Video](https://github.com/Lightricks/LTX-Video) ‚Äî **Lightricks** ‚Äî Real-time generation (faster than playback on H100). Lightweight architecture. Apache 2.0. [Paper](https://arxiv.org/abs/2501.00103)
- [CogVideoX](https://github.com/THUDM/CogVideo) ‚Äî **Tsinghua/Zhipu** ‚Äî 2B and 5B versions. 6s at 480p. Expert Transformer architecture. Apache 2.0. [Paper](https://arxiv.org/abs/2408.06072)
- [Mochi 1](https://github.com/genmoai/mochi) ‚Äî **Genmo** ‚Äî Asymmetric diffusion transformer. Good motion quality. Apache 2.0. [Blog](https://www.genmo.ai/blog)
- [Pyramid Flow](https://github.com/jy0205/Pyramid-Flow) ‚Äî **ByteDance** ‚Äî Multi-resolution generation with pyramid architecture. 10s at 768p. MIT. [Paper](https://arxiv.org/abs/2410.05954)

### 2024

- [Open-Sora](https://github.com/hpcaitech/Open-Sora) ‚Äî **HPC-AI Tech** ‚Äî Community reproduction of Sora-like architecture. Up to 16s at 720p. Apache 2.0. Active development.
- [Open-Sora-Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan) ‚Äî **PKU** ‚Äî Alternative open implementation. Focus on efficient training. MIT license.
- [Stable Video Diffusion](https://github.com/Stability-AI/generative-models) ‚Äî **Stability AI** ‚Äî The original open I2V model. 4s at 576√ó1024. Foundation for many community models.
- [AnimateDiff](https://github.com/guoyww/AnimateDiff) ‚Äî **ByteDance** ‚Äî Motion module plugin for Stable Diffusion. Works with existing SD checkpoints and LoRAs. Apache 2.0. [Paper](https://arxiv.org/abs/2307.04725)
- [VideoCrafter2](https://github.com/AILab-CVC/VideoCrafter) ‚Äî **Tencent AI Lab** ‚Äî High-quality T2V with DiT backbone. 2s at 320√ó512. [Paper](https://arxiv.org/abs/2401.09047)
- [ModelScope T2V](https://modelscope.cn/models/iic/text-to-video-synthesis) ‚Äî **Alibaba DAMO** ‚Äî One of the first open T2V models. 2s at 256√ó256. Historical significance.

### Utilities & Frameworks

- [ComfyUI](https://github.com/comfyanonymous/ComfyUI) ‚Äî Node-based workflow editor. Supports all major open video models. The Swiss Army knife of AI video.
- [A1111 WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) ‚Äî With AnimateDiff extension for video generation.
- [Diffusers](https://github.com/huggingface/diffusers) ‚Äî Hugging Face library. Python API for all major video models. `pip install diffusers`.
- [LivePortrait](https://github.com/KwaiVGI/LivePortrait) ‚Äî **Kuaishou** ‚Äî Efficient portrait animation. Stitching and retargeting.
- [ToonCrafter](https://github.com/ToonCrafter/ToonCrafter) ‚Äî Cartoon video interpolation. Generate in-between frames.
- [ProPainter](https://github.com/sczhou/ProPainter) ‚Äî Video inpainting with flow-guided propagation. Object removal.
- [FILM](https://github.com/google-research/frame-interpolation) ‚Äî **Google** ‚Äî Frame interpolation for smooth slow-motion. Production quality.
- [RIFE](https://github.com/hzwer/RIFE) ‚Äî Real-time intermediate flow estimation. Fast frame interpolation.

## ComfyUI Workflows

Ready-to-use ComfyUI workflow files (.json) for video generation pipelines.

- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) ‚Äî Wan 2.1 T2V and I2V in ComfyUI. LoRA support, controlnet integration.
- [ComfyUI-HunyuanVideoWrapper](https://github.com/kijai/ComfyUI-HunyuanVideoWrapper) ‚Äî HunyuanVideo with memory optimization. Supports long video generation.
- [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved) ‚Äî Advanced AnimateDiff integration. Motion LoRAs, camera controls, prompt travel.
- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite) ‚Äî Load, combine, split, and export videos. Essential utility nodes.
- [ComfyUI-Frame-Interpolation](https://github.com/Fannovel16/ComfyUI-Frame-Interpolation) ‚Äî FILM and RIFE nodes for smooth slow-motion and frame interpolation.
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes) ‚Äî Utility nodes for batch processing, scheduling, and advanced workflows.
- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) ‚Äî ControlNet integration for guided video generation. Depth, pose, edge control.
- [ComfyUI-LivePortraitKJ](https://github.com/kijai/ComfyUI-LivePortraitKJ) ‚Äî LivePortrait in ComfyUI. Portrait animation from reference image + driving video.
- [ComfyUI-CogVideoXWrapper](https://github.com/kijai/ComfyUI-CogVideoXWrapper) ‚Äî CogVideoX T2V and I2V. LoRA fine-tuning support.
- [ComfyUI-LTXVideo](https://github.com/Lightricks/ComfyUI-LTXVideo) ‚Äî Official LTX-Video nodes. Fast generation with quality presets.
- [comfyui-reactor-node](https://github.com/Gourieff/comfyui-reactor-node) ‚Äî Face swap for video generation. Consistent character faces across frames.
- [ComfyUI-IP-Adapter](https://github.com/cubiq/ComfyUI_IPAdapter_plus) ‚Äî Image prompt adapter for style and character consistency in video.
- [Steerable Motion](https://github.com/banodoco/Steerable-Motion) ‚Äî Controlled motion generation. Camera movement and subject motion presets.
- [ComfyUI-depth-fm](https://github.com/kijai/ComfyUI-depth-fm) ‚Äî Depth estimation for video. Use depth maps to guide generation.
- [ComfyUI Workflows Collection](https://openart.ai/workflows/all?category=video) ‚Äî Community-shared workflows on OpenArt. Hundreds of ready-to-use pipelines.

## APIs & SDKs

Developer tools for integrating AI video generation into applications.

### Commercial APIs

- [Runway API](https://docs.runwayml.com) ‚Äî Gen-3 Alpha T2V and I2V. Per-second billing. REST API with webhooks.
- [Luma API](https://docs.lumalabs.ai) ‚Äî Dream Machine via API. Fast generation, competitive pricing.
- [Kling API](https://docs.qingque.cn) ‚Äî Kling 2.0 via Kuaishou's platform. Rate-limited free tier.
- [Minimax API](https://www.minimaxi.com/platform) ‚Äî Hailuo video generation API. Good documentation.
- [Stability AI API](https://platform.stability.ai) ‚Äî Stable Video Diffusion and image models. Credits-based.
- [D-ID API](https://docs.d-id.com) ‚Äî Talking head generation API. Real-time streaming support.
- [HeyGen API](https://docs.heygen.com) ‚Äî Avatar video generation. Webhook callbacks for async processing.
- [Synthesia API](https://docs.synthesia.io) ‚Äî Enterprise avatar video API. Custom avatar support.

### Open-Source Hosting

- [Replicate](https://replicate.com) ‚Äî One-line API for 100+ open video models. `replicate.run("wan-ai/wan2.1-t2v-14b")`.
- [fal.ai](https://fal.ai) ‚Äî Optimized inference with queue management. Fast cold starts.
- [Together AI](https://www.together.ai) ‚Äî Dedicated GPU instances for video models. Batch processing support.
- [Modal](https://modal.com) ‚Äî Serverless GPU compute. Define video pipelines in Python, deploy instantly.
- [Banana](https://www.banana.dev) ‚Äî GPU inference as a service. Deploy custom video models.
- [Baseten](https://www.baseten.co) ‚Äî Model serving platform. Auto-scaling for video generation workloads.

## Prompt Engineering

Templates and techniques for getting better results from AI video generators.

### Prompt Structure

The best video prompts follow a consistent structure:

```
[Camera movement], [Subject description], [Action/Motion], [Setting/Environment], [Lighting], [Style/Mood], [Technical specs]
```

### Camera Movement Prompts

```
"Slow dolly zoom into..." ‚Äî Creates Vertigo/Hitchcock effect
"Aerial drone shot sweeping over..." ‚Äî Establishing shots
"Handheld tracking shot following..." ‚Äî Documentary feel
"Smooth crane shot rising from...to reveal..." ‚Äî Cinematic reveal
"Static locked-off wide shot of..." ‚Äî Stable composition
"POV walking through..." ‚Äî Immersive first-person
"360-degree orbit around..." ‚Äî Product/character showcase
"Whip pan from...to..." ‚Äî Fast transitions
"Slow push-in on..." ‚Äî Building tension
"Pull-back reveal showing..." ‚Äî Surprise/scale reveal
```

### Style Modifiers

```
"cinematic, 35mm film grain, shallow depth of field" ‚Äî Film look
"anime style, cel-shaded, vibrant colors" ‚Äî Anime
"photorealistic, 8K, hyperdetailed" ‚Äî Maximum realism
"watercolor painting coming to life" ‚Äî Artistic
"vintage 1970s home video, VHS artifacts" ‚Äî Retro
"neon-lit cyberpunk city, rain reflections" ‚Äî Sci-fi
"stop-motion claymation, tactile textures" ‚Äî Animation
"black and white, high contrast, film noir" ‚Äî Noir
"miniature tilt-shift, toy-like" ‚Äî Miniature
"long exposure light trails, time-lapse" ‚Äî Abstract motion
```

### Effective Prompt Examples

**Cinematic Nature:**
```
Slow aerial drone shot descending through morning mist over an ancient redwood forest,
sunbeams piercing through the canopy, a deer looks up startled then bounds away,
golden hour lighting, shallow depth of field, shot on ARRI Alexa, cinematic color grading
```

**Product Showcase:**
```
Smooth 360-degree orbit around a luxury watch on a black marble surface,
dramatic rim lighting revealing metallic textures, subtle caustic reflections,
macro lens detail on the dial, studio lighting, commercial photography style
```

**Character Action:**
```
Tracking shot following a samurai walking through a bamboo forest in heavy rain,
water droplets in slow motion off the sword, atmospheric fog, backlit silhouette,
Akira Kurosawa style, desaturated with selective red accents, 24fps cinematic
```

**Abstract/Artistic:**
```
Ink drops falling into clear water in extreme slow motion, blooming into fractal patterns,
morphing from jellyfish shapes into galaxies, deep blue and gold color palette,
macro photography, shallow depth of field, meditative and ethereal mood
```

**Urban/Street:**
```
Hyperlapse through Tokyo streets at night, neon signs reflected in rain puddles,
crowds of people in motion blur, camera weaving through narrow alleyways,
emerging into the bright lights of Shibuya crossing, cyberpunk atmosphere
```

### Model-Specific Tips

| Model | Best For | Tips |
|-------|----------|------|
| Kling 2.0 | Human motion, faces | Be specific about expressions and gestures |
| Veo 2 | Complex scenes, physics | Describe physical interactions in detail |
| Sora 2 | Narrative sequences | Use temporal language ("first...then...finally") |
| Runway Gen-3 | Creative/artistic | Lean into style descriptions and lighting |
| Hailuo | Cinematic quality | Reference specific film styles and directors |
| Pika | Quick iterations | Keep prompts shorter and more focused |
| Wan 2.1 | Open-source base | Detailed scene descriptions work best |

### Negative Prompts

Common negative prompts to improve quality:

```
"blurry, distorted, deformed, low quality, pixelated, watermark, text overlay,
extra fingers, mutated hands, poorly drawn face, out of frame, duplicate,
morphing artifacts, temporal flickering, inconsistent lighting"
```

## Benchmark & Comparisons

Systematic comparisons of video generation models.

- [Artificial Analysis Video Arena](https://artificialanalysis.ai/text-to-video/arena) ‚Äî Elo-based ranking from blind human comparisons. Most trusted independent benchmark.
- [VBench](https://github.com/Vchitect/VBench) ‚Äî Comprehensive benchmark suite. 16 dimensions including subject consistency, motion smoothness, aesthetic quality. [Paper](https://arxiv.org/abs/2311.17982)
- [EvalCrafter](https://github.com/EvalCrafter/EvalCrafter) ‚Äî Multi-aspect evaluation: visual quality, content accuracy, motion quality, temporal consistency. [Paper](https://arxiv.org/abs/2310.11440)
- [FETV](https://github.com/llyx97/FETV) ‚Äî Fine-grained Evaluation of Text-to-Video. Temporal and spatial alignment scoring. [Paper](https://arxiv.org/abs/2311.01813)
- [VideoScore](https://github.com/TIGER-AI-Lab/VideoScore) ‚Äî Automated scoring model trained on human preferences. Useful for pipeline evaluation.
- [T2V-CompBench](https://github.com/KaiyueSun98/T2V-CompBench) ‚Äî Compositional text-to-video generation benchmark. Tests multi-object scenes. [Paper](https://arxiv.org/abs/2407.14505)

## Research Papers

Key papers advancing the field. Sorted by date, newest first.

### Foundation Models (2025-2026)

- **[Wan 2.1](https://arxiv.org/abs/2503.20314)** ‚Äî Open and Advanced Large-Scale Video Generative Model ‚Äî *Alibaba, Mar 2025*
- **[Step Video](https://arxiv.org/abs/2502.10248)** ‚Äî T2V with Reinforcement Learning from Human Feedback ‚Äî *StepFun, Feb 2025*
- **[Cosmos](https://arxiv.org/abs/2501.03575)** ‚Äî World Foundation Model for Physical AI ‚Äî *NVIDIA, Jan 2025*
- **[HunyuanVideo](https://arxiv.org/abs/2412.03603)** ‚Äî Systematic Framework For Large Video Generation ‚Äî *Tencent, Dec 2024*
- **[Movie Gen](https://arxiv.org/abs/2410.13720)** ‚Äî A Cast of Media Foundation Models ‚Äî *Meta, Oct 2024*
- **[CogVideoX](https://arxiv.org/abs/2408.06072)** ‚Äî Text-to-Video with Expert Transformer ‚Äî *Zhipu AI, Aug 2024*
- **[Pyramid Flow](https://arxiv.org/abs/2410.05954)** ‚Äî Video Generation via Pyramid Flow Matching ‚Äî *ByteDance, Oct 2024*
- **[LTX-Video](https://arxiv.org/abs/2501.00103)** ‚Äî Realtime Video Latent Diffusion ‚Äî *Lightricks, Jan 2025*

### Architecture Innovations (2023-2024)

- **[Sora Technical Report](https://openai.com/research/video-generation-models-as-world-simulators)** ‚Äî Video Generation Models as World Simulators ‚Äî *OpenAI, Feb 2024*
- **[Stable Video Diffusion](https://arxiv.org/abs/2311.15127)** ‚Äî Scaling Latent Video Diffusion ‚Äî *Stability AI, Nov 2023*
- **[VideoPoet](https://arxiv.org/abs/2312.14125)** ‚Äî Large Language Model for Zero-Shot Video Generation ‚Äî *Google, Dec 2023*
- **[AnimateDiff](https://arxiv.org/abs/2307.04725)** ‚Äî Animate Personalized Text-to-Image Models ‚Äî *ByteDance, Jul 2023*
- **[Gen-2](https://arxiv.org/abs/2302.03011)** ‚Äî Structure and Content-Guided Video Synthesis ‚Äî *Runway, Feb 2023*
- **[Make-A-Video](https://arxiv.org/abs/2209.14792)** ‚Äî Text-to-Video without Text-Video Data ‚Äî *Meta, Sep 2022*
- **[Imagen Video](https://arxiv.org/abs/2210.02303)** ‚Äî High Definition Video with Diffusion Models ‚Äî *Google, Oct 2022*

### Controllable Generation

- **[DragAnything](https://arxiv.org/abs/2403.07420)** ‚Äî Motion Control for Anything via Entity Representation ‚Äî *Mar 2024*
- **[MotionCtrl](https://arxiv.org/abs/2312.03641)** ‚Äî Unified and Flexible Motion Controller ‚Äî *Dec 2023*
- **[AnimateAnything](https://arxiv.org/abs/2311.12886)** ‚Äî Fine-Grained Open Domain Image Animation ‚Äî *Nov 2023*
- **[DragNUWA](https://arxiv.org/abs/2308.08089)** ‚Äî Fine-Grained Control in Video Generation ‚Äî *Aug 2023*
- **[Control-A-Video](https://arxiv.org/abs/2305.13840)** ‚Äî Controllable Text-to-Video with ControlNet ‚Äî *May 2023*
- **[VideoComposer](https://arxiv.org/abs/2306.02018)** ‚Äî Compositional Video Synthesis with Motion Controlability ‚Äî *Jun 2023*

### Video Editing & Enhancement

- **[TokenFlow](https://arxiv.org/abs/2307.10373)** ‚Äî Consistent Diffusion Features for Video Editing ‚Äî *Jul 2023*
- **[Rerender A Video](https://arxiv.org/abs/2306.07954)** ‚Äî Zero-Shot Text-Guided Video-to-Video Translation ‚Äî *Jun 2023*
- **[Tune-A-Video](https://arxiv.org/abs/2212.11565)** ‚Äî One-Shot Tuning of Image Diffusion Models for T2V ‚Äî *Dec 2022*
- **[FateZero](https://arxiv.org/abs/2303.09535)** ‚Äî Fusing Attentions for Zero-shot Text-based Video Editing ‚Äî *Mar 2023*
- **[Text2Video-Zero](https://arxiv.org/abs/2303.13439)** ‚Äî Text-to-Image Diffusion Models are Zero-Shot Video Generators ‚Äî *Mar 2023*

### Surveys & Overviews

- **[A Survey on Video Diffusion Models](https://arxiv.org/abs/2310.10647)** ‚Äî Comprehensive survey of diffusion-based video generation ‚Äî *Oct 2023*
- **[Video Generation Models as World Simulators](https://openai.com/research/video-generation-models-as-world-simulators)** ‚Äî OpenAI's vision for video models ‚Äî *Feb 2024*
- **[Diffusion Models for Video Generation](https://arxiv.org/abs/2407.07508)** ‚Äî Tutorial and review ‚Äî *Jul 2024*

## Tutorials & Courses

### Getting Started

- [How to Create AI Videos: A Beginner's Guide](https://heyvid.ai/blog) ‚Äî Covers all major tools with step-by-step instructions and prompt examples
- [Runway Academy](https://academy.runwayml.com) ‚Äî Official courses from Runway. Free, covers Gen-3 Alpha and creative workflows
- [Stability AI Video Guide](https://platform.stability.ai/docs/guides/video) ‚Äî Official documentation for Stable Video Diffusion API

### ComfyUI Video

- [ComfyUI Video Workflows Guide](https://comfyanonymous.github.io/ComfyUI_examples/video/) ‚Äî Official examples for video generation nodes
- [Banodoco Steerable Motion Tutorial](https://github.com/banodoco/Steerable-Motion) ‚Äî Controlled camera and subject motion in ComfyUI
- [AnimateDiff in ComfyUI](https://stable-diffusion-art.com/animatediff-comfyui/) ‚Äî Step-by-step setup guide with example workflows

### Developer Integration

- [Replicate Video Generation Guide](https://replicate.com/docs/guides/video-generation) ‚Äî API integration tutorial with Python and Node.js examples
- [fal.ai Video Models Quickstart](https://fal.ai/docs/quickstart) ‚Äî Deploy video generation in 5 minutes
- [Building a Video Generation Pipeline](https://huggingface.co/docs/diffusers/using-diffusers/text-to-video) ‚Äî Hugging Face Diffusers library tutorial

### Advanced Techniques

- [LoRA Training for Video Models](https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/docs/training.md) ‚Äî Fine-tune Wan 2.1 on custom data
- [Video Upscaling Pipeline](https://stable-diffusion-art.com/video-upscale/) ‚Äî Combine generation + upscaling for maximum quality
- [Multi-Shot Video Storytelling](https://www.youtube.com/results?search_query=ai+video+multi+shot+workflow) ‚Äî Creating coherent narratives across multiple generated clips

### YouTube Channels

- [Olivio Sarikas](https://www.youtube.com/@OlivioSarikas) ‚Äî In-depth AI video tool reviews and tutorials
- [Matt Wolfe](https://www.youtube.com/@MattVidPro) ‚Äî AI tools overview and comparisons
- [Aitrepreneur](https://www.youtube.com/@Aitrepreneur) ‚Äî Open-source AI video workflows
- [Nerdy Rodent](https://www.youtube.com/@NerdyRodent) ‚Äî Technical deep dives into video generation models
- [Sebastian Kamph](https://www.youtube.com/@sebastiankamph) ‚Äî ComfyUI and Stable Diffusion video workflows
- [Theoretically Media](https://www.youtube.com/@TheoreticMedia) ‚Äî AI filmmaking and creative applications

## Communities & Forums

- [r/aivideo](https://reddit.com/r/aivideo) ‚Äî Main Reddit community for AI video generation. Prompt sharing, model comparisons, tutorials.
- [r/StableDiffusion](https://reddit.com/r/StableDiffusion) ‚Äî Includes AnimateDiff and video generation discussions. Large community (1M+ members).
- [r/RunwayML](https://reddit.com/r/RunwayML) ‚Äî Runway-specific community. Gen-3 Alpha tips and creative showcases.
- [r/singularity](https://reddit.com/r/singularity) ‚Äî AI advancement discussions. First place new model demos appear.
- [Civitai](https://civitai.com) ‚Äî Model hub. Video LoRAs, AnimateDiff motion modules, community workflows.
- [Hugging Face](https://huggingface.co/models?pipeline_tag=text-to-video) ‚Äî Open model hub. Papers, model cards, and Spaces demos.
- [Runway Discord](https://discord.gg/runway) ‚Äî Official Runway community. Challenges, feedback, creative showcases.
- [Pika Discord](https://discord.gg/pika) ‚Äî Official Pika community. Prompt sharing and feature discussions.
- [ComfyUI Discord](https://discord.gg/comfyorg) ‚Äî Technical support and workflow sharing for ComfyUI.
- [Banodoco Discord](https://discord.gg/banodoco) ‚Äî Steerable Motion and open-source video generation community.
- [Deforum Discord](https://discord.gg/deforum) ‚Äî Audio-reactive and animated Stable Diffusion community.
- [AI Filmmaking Discord](https://discord.gg/) ‚Äî Independent filmmakers using AI tools. Collaborative projects.

## Multi-Language Resources

AI video generation is a global field. Here are localized resources and tools:

| Language | Resource | Notes |
|----------|----------|-------|
| üá∫üá∏ English | [HeyVid AI](https://heyvid.ai) | Full platform, all models |
| üá™üá∏ Espa√±ol | [HeyVid AI (Espa√±ol)](https://heyvid.ai/es) | Interfaz completa en espa√±ol |
| üá´üá∑ Fran√ßais | [HeyVid AI (Fran√ßais)](https://heyvid.ai/fr) | Interface compl√®te en fran√ßais |
| üá©üá™ Deutsch | [HeyVid AI (Deutsch)](https://heyvid.ai/de) | Vollst√§ndige deutsche Oberfl√§che |
| üáØüáµ Êó•Êú¨Ë™û | [HeyVid AI (Êó•Êú¨Ë™û)](https://heyvid.ai/ja) | Êó•Êú¨Ë™û„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„ÇπÂØæÂøú |
| üá∞üá∑ ÌïúÍµ≠Ïñ¥ | [HeyVid AI (ÌïúÍµ≠Ïñ¥)](https://heyvid.ai/ko) | ÌïúÍµ≠Ïñ¥ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏßÄÏõê |
| üáßüá∑ Portugu√™s | [HeyVid AI (Portugu√™s)](https://heyvid.ai/pt) | Interface completa em portugu√™s |
| üáÆüáπ Italiano | [HeyVid AI (Italiano)](https://heyvid.ai/it) | Interfaccia completa in italiano |
| üá∑üá∫ –†—É—Å—Å–∫–∏–π | [HeyVid AI (–†—É—Å—Å–∫–∏–π)](https://heyvid.ai/ru) | –ü–æ–ª–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º |
| üá®üá≥ ÁÆÄ‰Ωì‰∏≠Êñá | [HeyVid AI (‰∏≠Êñá)](https://heyvid.ai/zh) | ÂÆåÊï¥‰∏≠ÊñáÁïåÈù¢ |
| üáπüáº ÁπÅÈ´î‰∏≠Êñá | [HeyVid AI (ÁπÅÈ´î)](https://heyvid.ai/zh-Hant) | ÁπÅÈ´î‰∏≠Êñá‰ªãÈù¢ |
| üáπüá∑ T√ºrk√ße | [HeyVid AI (T√ºrk√ße)](https://heyvid.ai/tr) | Tam T√ºrk√ße aray√ºz |
| üáÆüá© Indonesia | [HeyVid AI (Indonesia)](https://heyvid.ai/id) | Antarmuka Bahasa Indonesia |
| üáπüá≠ ‡πÑ‡∏ó‡∏¢ | [HeyVid AI (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)](https://heyvid.ai/th) | ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ü‡∏ã‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ |
| üáµüá± Polski | [HeyVid AI (Polski)](https://heyvid.ai/pl) | Pe≈Çny interfejs po polsku |
| üá≥üá± Nederlands | [HeyVid AI (Nederlands)](https://heyvid.ai/nl) | Complete Nederlandse interface |
| üá©üá∞ Dansk | [HeyVid AI (Dansk)](https://heyvid.ai/da) | Komplet dansk gr√¶nseflade |
| üá≥üá¥ Norsk | [HeyVid AI (Norsk)](https://heyvid.ai/nb) | Komplett norsk grensesnitt |

---

## Contributing

Contributions are welcome! Please read the [contributing guidelines](CONTRIBUTING.md) first.

Ways to contribute:
- Add a tool or resource that's missing
- Update outdated information (pricing, model versions, links)
- Share prompt templates that work well
- Add ComfyUI workflows
- Fix broken links
- Translate sections into other languages

## License

[![CC0](https://licensebuttons.net/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

This work is released under CC0 1.0 Universal. You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.

---

<div align="center">

**‚≠ê Star this repo to help others discover it!**

*Maintained by the community. Last verified: February 2026.*

</div>
